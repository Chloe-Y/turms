# 系统资源管理

内存与CPU资源对服务端的重要性不言而喻，Turms各模块都比较极致地使用内存与CPU，具体可参考各模块实现的文档与代码。而在另一方面，为保证服务端的正常运行，其内部也提供了一套健康检测机制，该机制配合上层的“拒绝服务”机制，以尽最大努力保证服务端能够正常运行。

Turms提供系统资源监控配置类：`im.turms.server.common.property.env.common.healthcheck.HealthCheckProperties`，来允许用户配置可用内存占用率与CPU占用率。Turms服务端的`HealthCheckManager`会持续检测可用物理内存与CPU占用率，如果检测到可用物理内存过低或CPU占用率过高，则会：

* 将自身在服务注册中心的`isHealthy`信息标记为`false`。由于RPC发送端只会从`isHealthy`为`true`的服务端中，挑选RPC的响应服务端，因此能实现类似背压的效果
* 拒绝对外提供服务。具体而言：如果是turms-gateway服务端，则拒绝新会话的建立与用户请求的处理；如果是turms-service服务端，则拒绝处理turms-gateway服务端发来的RPC请求（注意：就算处于“不健康”状态，turms-service仍然会为管理员API提供服务）

## 内存管理

### JVM基础内存知识

JVM HotSpot虚拟机的内存区域可以划分为：

* 堆内存（Heap Memory）：Eden区、Survivor区、老年代（Old Generation）

* 非碓内存（Non-heap Memory）

  * 直接内存（Direct Memory）：Direct Buffer Pool
  * JVM内部内存（JVM Specific Memory）：本地方法栈、元空间、Code Cache等

  特别注意：通过函数`java.lang.management.MemoryMXBean#getNonHeapMemoryUsage`获得的`NonHeapMemory`并不包括`Direct Buffer Pool`（直接内存缓存池）。具体而言，该函数在JDK 17中所指的内存空间为：

  * CodeHeap 'non-nmethods'
  * CodeHeap 'non-profiled nmethods'
  * CodeHeap 'profiled nmethods'
  * Compressed Class Space
  * Metaspace

参考文档：[How to Monitor VM Internal Memory](https://docs.oracle.com/en/java/javase/17/troubleshoot/diagnostic-tools.html#GUID-FB0581EA-2F91-4093-B2FA-46687F7AB081)

### 可控内存（Managed Memory）的使用

Turms服务端的可控内存指的是`堆内存（Heap Memory）`与`直接内存（Direct Memory）`这两块区域。

#### 堆内存

##### 实践意义

堆内存的实践意义比较容易理解，就是尽可能配置大的堆内存，以减少GC次数与`stop-the-world`事件的发生。

##### 配置

JVM默认的堆配置如下：

```
-XX:MaxRAMPercentage=75
-XX:InitialRAMPercentage=75
```

其中：

* `InitialRAMPercentage`与`MaxRAMPercentage`指定了需要reserve内存的大小，但Turms服务端访问该内存区域时仍会发生缺页异常。虽然JVM可以通过配置`AlwaysPreTouch`，将reserved内存直接转换成committed内存，来避免服务端在运行时发生缺页异常。但因为开启该选项后，服务端很难监控真正被使用了的堆内存，因此目前不推荐添加该配置。
* `InitialRAMPercentage`与`MaxRAMPercentage`设成一样的值主要是为了尽可能保证内存的连续性，避免服务端因为内存扩容与缩容，反复进行GC与`stop-the-world`操作。
* 堆内存没有配置为接近100%的值，这是为了把剩余的物理内存让给JVM自身的堆外内存（如占最大头的直接内存、CodeCache、Metaspace等）、系统内核（如维护TCP连接时的缓冲区）与边车服务（如：日志采集服务）使用。

另外，推荐生产环境不要给Turms服务端分配超过32GB内存。因为：

* 开启JVM的指针压缩技术，以减少不必要的内存占用
* 避免单个服务端承载太多负荷，在停机时减缓惊群效应，提升用户体验

#### 直接内存

下文所述的所有`直接内存`在实际代码中，都是由`PooledByteBufAllocator.DEFAULT`分配，即它们都是被Netty缓存与管理的直接内存。

##### 实践意义

直接内存的容量上限影响Turms服务端在同一时刻能够处理的客户端请求与管理员API请求的峰值

##### 主要使用方

* TCP的读写操作。如基于Netty的：第三方依赖`mongo-driver-java`与`Lettuce`等驱动；Turms服务端自身面向客户端的TCP/HTTP/UDP服务端实现
* 日志打印。Turms自研的日志打印实现直接将Java基础数据写入直接内存块中，再将其写入文件描述符

  特别一提的是：Java常规异常日志格式所需内存是非常夸张，异常的一个Cause格式化后约占1000~2000字符，换言之，一个异常只要多几个Causes，打印一个异常就需要花费几MB的内存。

换言之，基本上所有需要系统内核访问的内存区域，我们都是直接使用直接内存，以避免无意义的堆内存拷贝。

注意：在Linux系统中，Turms使用的直接内存仍处于用户空间内，因此将直接内存写入设备（如网卡与硬盘）时，仍需要进行用户空间到内核空间、内核空间到设备的两次拷贝，而这两次拷贝操作是上层服务端无法避免的。

##### 生命周期

因为在Turms服务端中，直接内存的生命周期与客户端请求与管理员API请求的生命周期高度一致，一块直接内存通常只会在一个请求的部分或全部生命周期中存在。具体而言，其生命周期大体如下：

* 一个请求的生命周期开始于Netty对TCP字节流进行切割的阶段，Netty根据varint编码的header（其值表示的Payload长度），来对TCP字节流进行切割，而当这块内存被切割出来时（注意：这里没有发生内存拷贝），这块代表请求的直接内存的生命周期也就此开始了。

* 在Turms服务端将这块内存解析成具体的请求模型之后，Turms会判断该类型的请求是否需要使用代表它自己的直接内存。如果该请求的处理逻辑不需要使用这块内存，则这块内存会被马上回收回Netty的内存缓存池中。否则，诸如“转发用户消息”这样的请求需要使用这块内存，则该块内存不会被马上回收。接着Turms会对该请求进行业务逻辑处理。

* 在业务处理的过程中，可能会涉及到其他网络I/O操作（如向MongoDB/Redis发请求）或日志打印操作，这两类操作都需要从Netty管理的内存缓冲池中取出新的直接内存块，以进行MongoDB/Redis客户端请求的编码与响应解码操作、或日志打印操作。

* 等Turms服务端最终将请求响应的直接内存Flush到网卡后，除了代表日志记录的直接内存外，该过程所涉及的其他直接内存也都会被回收。

  唯一一种例外情况是：如果一个请求的直接内存需要转发给多个客户端，那么Turms会通过引用计数器将该请求的生命周期与其直接内存的生命周期分离，以保证能够将同一块直接内存转发给多个客户端，以避免内存拷贝。

  注意：
  
  1. 上文所述的`直接内存回收`并不是将内存回收给系统，而是回收回由Netty管理的内存池中，该内存并不会在这时被真正释放。
  2. 直接内存主要是通过：当Pooled ByteBuf被`release`时，Netty会检测其所属Chunk是否已闲置（使用率为0%）。如果是，则通过函数`io.netty.buffer.PoolArena#destroyChunk`真正释放该内存。

由于该生命周期的存在，堆内存与直接内存的真实使用率其实具有关联性。堆内存的增长主要是因为Turms服务端接收到了客户端请求或管理员API请求后处理的一系列逻辑。而在一过程中，直接内存的使用率增高是因为请求的解码与响应的编码、逻辑中的网络I/O操作的编解码与日志打印。当请求的生命周期结束时，堆内存与直接内存也就都可以被回收了。

### 内存健康检测

#### 配置

配置类：`im.turms.server.common.property.env.common.healthcheck.MemoryHealthCheckProperties`

如上文所述，要想让运维人员准确评估服务端应该使用多少内存其实是非常困难，甚至不现实的事，尤其是一些关键系统内核（如TCP连接）所占内存是动态变化的，因此`MemoryHealthCheckProperties`除了提供诸如`maxAvailableMemoryPercentage`与`maxAvailableDirectMemoryPercentage`这样限定Turms服务端可使用内存上限的配置，同时也提供了`minFreeSystemMemoryBytes`这一配置，让Turms服务端能够实时检测系统的可用物理内存，并尽最大努力预留这些内存出来。

#### 内存监控实现——MemoryHealthChecker

作用：

* 检测到系统物理内存不足时，通知上层服务拒绝处理用户会话与请求，以尽最大努力保证不会耗尽物理内存，并避免使用Swap内存
* 如果检测到系统物理内存不足时，且已用堆内存超过`heapMemoryGcThresholdPercentage`，则调用`System.gc()`来建议JVM进行Full GC

特别注意

* 如上文所述，直接内存的生命周期与请求的生命周期高度一致，因此就算`MemoryHealthChecker`检测到了`已用总内存已经超过XX`，它也不会主动尝试去释放直接内存，而是等待Netty内部的内存管理机制对其进行释放
* 综上，尽管Turms服务端会尽最大努力不去耗尽物理内存，但对于极端突发的大量请求，Turms服务端还是有可能会耗尽物理内存，此时会采用Swap内存。如果Swap内存被系统关闭或Swap内存不足，则Turms服务端将直接抛出`OutOfMemoryError`异常。因此我们可以把使用Swap内存当作最后一道防线，故非常不推荐在生产环境中关闭Swap内存。

### 关于Valhalla

Java的内存占用一直为人所诟病，诸如一个Integer对象所存放的对象头所需的内存空间大于实际int数据数倍。相比很多追求性能优化（甚至是寄存器级别的优化）的C++服务端项目（如Nginx、Redis），由于Java自身的设计缺陷与保守，它对内存的浪费就让人感觉有些“自暴自弃”了，并且更糟糕的是这样的精神也传导给了整个Java生态圈。通过阅读源码，能发现很多知名Java项目也是“功能能用，功能写着舒服，性能差不多就行，反正JVM会帮忙GC”的态度，诸如可以很容易做Cache的地方不Cache、基础数据结构乱用、反复内存拷贝（如最常见的`String`与`StringBuilder`在实践中，通常来来回回拷贝很多次，源码让人触目惊心），关于这点我们已经在其他章节重点讲解了，故不赘述。

而Valhalla项目则对现有的Java Object体系进行了重构。原有的`Object`在新的Java体系中叫做`IdentityObject`，而新体系下的`Object`则成了`IdentityObject`与`ValueObject`的父类（注意：Valhalla团队尚未定稿，因此概念可能还会变），其中`ValueObject`让用户能够自定义性能如传统Java八大基础类型一样高效的数据结构，无需对象头、访问时无需通过指针查找、栈上分配，甚至直接存储在CPU寄存器之中。等Valhalla项目发布Preview版本后，我们将引入`ValueObject`，并且由于我们已等待该项目数年，非常熟悉其设计，故可在一周内完成适配与测试工作。这也是我们会为`Preview`特性开绿灯的唯一特性。

## 线程

由于Turms服务端不存在阻塞I/O，诸如RPC、MongoDB与Redis的网络请求都是基于Netty异步实现的。如果更往下看，在Linux系统上，即都为epoll相关操作，因此服务端所需的线程数远远少于传统Java Web应用。以16核CPU为例，turms-gateway与turms-service的线程数峰值的范围约在80~150（含JVM内部线程）之间，具体峰值数要根据服务器的CPU内核数与所运行的服务端个数（如一个turms-gateway可以同时启动TCP/WebSocket/UDP服务端）而定。

特别值得一提的是：Turms的线程峰值数与同时在线用户规模与请求QPS无关。

补充：正因为Turms服务端自身使用的线程数相比CPU核数而言并不算多，因此在个别代码中我们直接使用`ThreadLocal`缓存一些相对大且线程不安全的对象，并且相比传统服务端，Turms也极大地减少了线程上下文切换带来的开销

### CPU健康监控

配置类：`im.turms.server.common.property.env.common.healthcheck.CpuHealthCheckProperties`

作用：监控CPU使用率，如果N次检测到CPU使用率超过阈值，则将节点的`isHealthy`设为`false`，并与其他节点共享该状态，同时拒绝提供服务，直到CPU使用率健康。具体配置见上述的配置类。

### Turms线程列表

TODO

### 线程模型

TODO

### 关于Loom项目

#### 背景

很多相对长寿的技术方案一方面即得益于其丰富的生态而长寿，另一方面又因为其丰富的生态而尾大不掉，由于不能顺应时代发展，而最终退出历史舞台。而在Java生态中，各种技术方案的阻塞实现其实就是危及Java在新时代发展的一大拦路虎。其中，JDBC阻塞实现就是Java异步生态实现的最大障碍，Turms没有采用传统SQL数据库的原因之一就是：当时的Java生态圈没有成熟的异步JDBC实现，甚至一些项目因此不以Java立项，而改用Go或C#等语言，只留下一句“Java的线程模型不够“云原生”，生态圈太落后”。

而Loom项目的革命性就在于它正式地将协程（Virtual Thread）引入了Java的世界，让庞大且保守的Java生态中的“阻塞”调用也支持异步执行。

#### Turms项目对Loom项目的态度

尽管上面说了Loom项目的革命之处，但Turms项目未来也不会采用Loom项目提供的协程，因为对于Turms服务端项目来说，协程只能增加新问题（如栈拷贝），并且不能解决已有的问题。并且，如果我们想使用协程，我们在立项Turms服务端时，直接就可以把Java排出去了，因为就算Loom正式发布了，其生态也要经过数年才能成熟，其坑还要无数人去踩。具体原因如下：

* 如上所说协程的革命性在于其试图解决Java生态已经重度使用阻塞API的现状，让同步的代码以异步的方式执行。但Turms服务端没有阻塞I/O，协程的革命性在Turms服务端这发挥不了作用。且如果第三方库如果使用了阻塞I/O，那我们通常会对其作者的技术水平产生怀疑，并不会使用其实现。

* Loom项目引入的有栈协程在suspend的时候需要保存调用栈，但这对Turms服务端来说就多此一举了，因为Turms服务端没有阻塞I/O，不需要suspend。尽管栈数据能解决reactor-core的一大致命缺点“异常的栈信息基本没用，很难Debug”，但reactor-core在Turms服务端的优化下已经克服了这个缺点（具体见下文`补充：reactor-core的缺点`）。我们在看推广Loom项目的文章时，经常会看到类似“就算开数万个协程，也只需占用这么一点内存”，但对于Turms服务端项目来说，Turms服务端只需开0个协程，多占用0字节的内存也能实现同样的效果。

* 协程的学习难度是“1+1>2”，其学习曲线其实高于`reactor-core`。说协程的学习难度是“1+1>2”是因为：开发者既要掌握线程，又要掌握协程，同时还要能保证以线程为模型的传统代码与第三方库要能正确地运行在协程当中，这对保守的Java生态来说，“保证在协程上也能正确运行”可不是件容易的事，而掌握`reactor-core`只需要最基本的线程知识。并且如果真要使用协程，那为何不采用Go这样天生支持协程的语言与生态。

  一些开发者会认为`reactor-core`的使用会比协程复杂，但这样的说法通常只是从初学者角度来看的。对于初级工程师而言，其实不管是协程，还是`reactor-core`，在不学习其原理的情况下，二者表面的使用其实都很简单，只是协程可以在Java层面保证了初级工程师很容易写出高性能的代码，而`reactor-core`最好要有高级工程师带着初级程序员写，否则代码可能维护性极差、甚至出现逻辑错误。但只要过了这短暂的初学阶段，学习协程就会面临刚刚提到的“学习难度1+1>2”的问题，而`reactor-core`只要求工程师掌握最基本的线程知识。

  另外，我们在编写Turms服务端代码的时候，从来不会考虑`该如何用reactor-core编写异步的代码`，如同很多开发者压根不会考虑`同步的代码该怎么写`。

* 协程对Java大生态的兼容性还是个问号。Loom项目自身其实还有很长的路要走，需要有大量的人来踩大量的坑。诸如在Java生态圈中，想要兼容Loom项目的作者都需要思考：在协程开启的情况下，如何正确使用ThreadLocal/Synchronized；日志的MDC还能正确运行吗；一些代码还能保证线程安全吗；Netty绑定线程的内存池还能正确运行吗等问题。

  也许5年、10年后，Java的老一代技术方案被淘汰了，新一代技术方案诞生了，但这也是猴年马月的事情了。更重要的是，折腾了这么久，系统层该调用什么系统函数还是没变，只是Java这层皮在不断的变化与折腾，而我们在做Turms服务端开发的时候，不太关心Java这层皮要做什么（具体原因见下文）。

* reactor-core不仅实现了异步调用，还提供更强的表达能力。如果我们想要知道一个链路的成功率、执行时间等度量数据，只需要调用`metrics(...)`这么一个函数。想要将切换数据流的执行线程，只需要执行`publishOn(...)`这么一个函数，线程的调度逻辑尽在掌握之中。

* 协程引入了新的抽象层（协程），而这层抽象层对于Turms服务端来说是多余的。作为软件工程师，我们知道“用户只需调用API，无需了解其内部实现”通常只是一句口号，实际上我们肯定是会大概读下其源码，了解其大致实现原理的，同时也是评估第三方库作者的技术水平可不可信。而我们在编写Turms服务端性能相关的关键代码时，通常是以系统调用的视角来写Java层的代码，Java只是帮忙给系统层调用套了层皮，而这层皮应该越“薄”越好，这样我们才能快速明白JVM到底是调用了什么syscall，以评估我们Java层的代码是否足够高效，还有没有优化的空间。

  如我们在使用Netty进行网络编程时，我们实际是会从操作系统的角度来看，Java这层皮要怎么高效调用与传递数据，才能最高效地使用Linux的epoll或Windows的IOCP。而在进行一些Java底层问题追踪或性能优化时，我们又需要调用`syscall`函数，查看操作系统层面是如何运行的。

  以具体案例来说，我们在编写Turms服务端的业务处理代码时，大致是这么思考的：当业务处理A线程将数据（如MongoDB/Redis请求）塞进Queue后，A线程就可以直接返回去处理其他业务请求了；网络I/O线程B线程在读到Queue中存在数据时，就会将数据通过Java`DirectByteBuffer`这层皮将堆外内存的地址与长度传递给JVM层的epoll适配接口；JVM再把数据flush到系统内核管理的TCP输出缓冲区，同时也要通过epoll不断接收TCP接收缓冲区的数据；等到线程C反序列完这些字节数据后，又把反序列化对象通过调用函数，完成一系列业务处理回调操作。综上，我们在编写Java代码时希望的是“所见即所得”，即：当我们希望线程这么交互时，操作系统的线程真得就这么进行交互，无需JVM再套一层协程，隐藏线程的调度逻辑。

额外补充：上文所述内容主要是针对Turms服务端项目而言的，Loom对于绝大多数Java项目来说还是利大于弊，尤其是第三方库作者不用再维护同步与异步两套实现。

#### 补充：reactor-core的缺点

如同我们在[关于依赖库的使用](https://turms-im.github.io/docs/for-developers/rules.html#%E5%85%B3%E4%BA%8E%E4%BE%9D%E8%B5%96%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8)章节已经提到过的，`reactor-core`这样的异步实现库最致命的缺点在于，当它结合一些提倡“多做封装、多做抽象、用户无需关闭实现逻辑”的依赖库时，开发者只能寄希望于服务端能够始终正常运行，否则一旦遇到了一个Bug，开发者很快就会情不自禁地产生一连串的疑问：“reactor-core这样的异步框架能用在生产环境吗？我连异常是哪里抛的都找不到，这样的代码真得能维护吗？”，部分项目甚至因此而采用其他语言，如Go，重构当前Java项目。

举例而言，控制台现在报了一个错“Netty提示：ByteBuf的引用计数已经为0，无法再次进行释放操作”。特别注意，这里并没有省去任何有用的日志信息，这就是开发者真正能从日志看到的所有有用信息。甚至这条日志去除了误导信息，即其堆栈信息。如果开发者根据堆栈信息去Debug，那永远都无法找到真实的Root Cause。而开发者能仅凭这行日志，知道为什么会发生这个异常，并定位出哪个模块导致的这个异常吗？这是Turms真实发生过的一个Bug，也是唯一一个花费6小时以上时间，去阅读Turms所有依赖的所有源码，并排查Root Cause的最难解决的Bug：[Memory leaks when Turms uses the previous buffer reference to release a recycled pooled buffer](https://github.com/turms-im/turms/issues/786)。

总之，想要用好`reactor-core`必须满足三个条件：

1. 所有关键代码必须可控，否则出错的时候只能寄希望于：

   * 第三方库的开发人员技术水平高，代码设计功底扎实。如果第三方依赖也是基于异步编程，那这个要求就更高，作者要能够预判上层开发者可能会遇到的异常，并通过异步手段，把异常抛给上层应用。

   * 第三方库不复杂，能快速阅读完相关源码。

     一个优秀的例子就是：reactor-netty。其开发人员的技术水平高，设计功底扎实。代码也比较精简，容易阅读。

2. 必须规范地传递异常与打印日志。就算是异步编程，只要规范地传递异常与打印日志，我们通过单条日志也能马上看出绝大部分Bug的缘由，只有个别Bug可能需要关联多条日志进行排查。如果做不到这点，出错时只能听天由命。

3. 团队里必须要有工程师熟练掌握异步编程。

只要缺少上面的一个条件，开发者迟早会遇到类似上述的“Netty提示：ByteBuf的引用计数已经为0，无法再次进行释放操作”这样难度的Bug，也因此对于一般的技术团队，我们更推荐Loom项目，而不是reactor-core。当然，更推荐的可能是切换编程语言。但Turms项目如今已经能满足上述条件，不再存在“异常难以Debug”的情况。

额外补充：在turms-admin管理系统开发的时候，我们通常也是尽量避免使用`await/async`，其原因是turms-admin最终会编译成ES5语法，而被`await/async`修饰的函数在`source map`关闭之后，非常难Debug，故尽量避免`await/async`。
