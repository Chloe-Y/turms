# 架构设计

## 背景

* 体量调整。调整的原因是：在小型与迷你型即时通讯领域，已经有像 [Rocket Chat](https://github.com/RocketChat/Rocket.Chat) 这样的优秀且功能完善的开源项目，即便Turms的早期架构设计中，在集群部署上有很大的性能与快速弹性扩展的优势，但由于其无独立接入层与独立用户状态管理集群的设计，导致在实际运用场景中Turms只能在中小型即时通讯领域游走，跟这些小型开源项目相比就难以大展身手了。同时也考虑到当前在全球范围内，尚未有能同时支持大中小型即时通讯场景的合格开源项目。因此做了Turms体量上的调整，也对架构进行了重构（v0.10.0）。
* 架构重构。Turms早期架构主要针对中小型场景设计，并假定服务端更新频率低且能接收集群弹性伸缩时带来的部分用户强制下线重连，以实现“整个架构的Overhead开销最小”以及“最大程度地简化运维人员的部署与运维工作”，因此采用了无网关层且无独立用户状态管理集群的架构设计。具体实现而言，以分槽算法为根本，辅之以内嵌的Hazelcast的ReplicatedMap分布式内存来实现整个集群会话管理与用户状态管理（可查看0.9.0版本代码）。但由于体量上的调整，因此最终没有用这样的架构设计。

## 当前架构

在新版的架构中（0.10.0），Turms基于标准的商业即时通讯架构做了针对Turms的架构设计。

* 宏观上，Turms接入了接入层（turms-gateway）与独立的用户状态管理集群（Redis），并充分利用云服务做设计（当然您也可以不用云服务，只是如果您使用云服务，您就无需进行额外的部署工作）
* 从具体的技术实现上来说，分布式内存的实现方案由原本的Turms基于Hazelcast自主实现转变为基于Redis集群实现。服务注册中心与配置中心的实现由原本的Turms基于Hazelcast自主实现转变为基于MongoDB集群以消息总线形式实现

但Turms的架构设计仍保留了“追求简约架构”（简约不简单）的传统。诸如：

* ~~不采用Kafka，而采用RSocket协议实现流量削峰~~
* 不采用专门的服务注册服务端（Consul、Zookeeper、Eureka等），而采用基于MongoDB的独立自研实现
* 不采用传统的微服务架构，而采用一种Turms独有的微服务架构设计。在开发与部署形式上，turms服务仍是单体架构，但turms支持通过动态配置，来实现零停机且实时地将一批服务端由原本的负责服务A，变成让其负责服务B。同时也支持在turms-gateway实时切换服务承载配重。在最终效果上看，传统微服务架构能做的，Turms的微服务架构也都能做，但远比传统微服务架构灵活

## 架构特性

### 通用架构特性

1. （敏捷性）支持在用户无感知的情况下，对Turms服务端进行停机更新，为快速迭代提供可能
2. （可伸缩性）无状态架构，Turms集群支持弹性扩展与异地多活的部署实现，用户可通过DNS就近接入
3. （可部署性）支持容器化部署，方便与云服务对接，以实现全自动化部署与运维
4. （可观测性）具备相对完善的可观测性体系设计，为业务统计与错误排查提供可能
5. （可拓展性）能同时支持中大型即时通讯场景，即便用户体量由小变大也无需重构（当然，对于大型运用场景还有很多优化的工作需要做，但当前架构不影响后期的无痛升级）
6. （安全性）提供限流防刷机制与用户/IP黑名单机制，以抵御大部分CC攻击
7. （简单性）核心架构“轻量”，方便学习与二次开发（原因请查阅 [Turms架构设计](https://turms-im.github.io/docs/for-developers/architecture.html)）
8. Turms使用MongoDB分片架构，以支持请求路由（如读写分离），同时也支持跨地域多活部署与数据主主同步，为大规模跨国部署提供实际操作的可能

## 架构说明

![参考架构图](https://raw.githubusercontent.com/turms-im/assets/master/turms/reference-architecture.png)

### 与其他IM项目的架构区别

* 在部分IM项目的架构设计中，它们会把turms-gateway的`会话管理`、`中转消息缓存`、`消息发送`独立分成三个服务，来实现业务解耦与流量削峰。但其相比Turms的架构而言，多增加了两个故障点，增加了开发与运维难度，且需要使用RPC操作，吞吐量也更差。

  在业务解耦方面，部分IM项目会通过`中转消息缓存的消息队列`来实现下游消费者异步消费消息来实现各种统计功能。但通过消费消息队列中的数据来实现消息的统计是很糟糕的设计，更全面与更专业的实现是分布式采集与分析业务日志，这点在[可观察体系](https://turms-im.github.io/docs/for-developers/observability.html)的日志小节有具体说明。而turms-gateway的`会话管理`与`消息发送`之间的逻辑并不复杂，解耦的意义不大，故没这方面需求。

  在流量削峰方面，如今早已是云服务的天下，弹性伸缩相比消息队列（如Kafka或RocketMQ）更适合实现流量削峰。弹性伸缩服务均提供资源监控功能，能根据各种系统指标（如CPU/内存使用率）与自定义的其他指标（如在线用户数）自动弹性伸缩，在资源闲置的时候又能自动释放，更符合现代运维模式。

  一些IM项目之所以强行进行解耦，引入消息队列，甚至是在同时在线用户数只有或不足数十万的时候就引入消息队列，只是因为部分开发人员为了给自己的简历润色，徒增项目所需技术栈，而对项目进行过度设计。

* 在部分IM项目的架构设计中，它们会把`会话管理`再拆成`网络连接管理`与`会话逻辑管理`两个服务，来实现停机更新`会话逻辑管理`服务时，客户端不需要断开与`网络连接管理`服务的连接。但考虑到turms-gateway几乎没什么会话业务逻辑，既有的业务逻辑也很固定，主要的业务逻辑都是在turms-service里实现的，因此turms-gateway很少有停机更新业务逻辑的需要。综上，把将网络连接与会话逻辑拆分成两个独立服务对Turms而言还为时过早，既增加了故障点，性能折损也大，又没什么收益，故Turms架构暂不对`会话管理`再进行拆分。

### 客户端访问服务端标准流程

该流程为客户端访问服务端标准流程，也是Turms架构实现水平扩展的过程，您可以根据实际情况进行调整。

* 当客户端需要与turms-gateway服务端建立TCP连接时，客户端可以通过`DNS服务`来查询接入层服务端域名对应的IP地址，而该IP地址指向`SLB/ELB服务`（通常基于LVS与Nginx）、`全球加速服务`、或`turms-gateway`，具体如何搭配要根据您实际应用的需求与规模而定。该DNS服务端可以配置一个或多个公网IP地址（生产环境中切勿配置真实IP地址，以缓解DDoS攻击），并通过轮询或其他策略返回给客户端一个IP地址。补充：

  * 无论Turms客户端使用的是TCP协议，还是上层的WebSocket协议，turms-gateway的上游服务（DNS/SLB等）都应该根据客户端IP地址进行TCP连接的负载均衡。

  * 并且，强烈建议您开启SLB服务的`Sticky Session`功能，让会话始终与一个turms-gateway服务端进行连接。这么做的好处是能缓解很大一部分DDoS攻击。因为turms-gateway提供客户端自动封禁机制，能够迅速检测并封禁本地有异常行为的IP或用户，但turms-gateway服务端之间同步封禁客户端数据默认时间间隔约10~15秒，因此如果关闭了`Sticky Session`功能，黑客就能利用封禁数据同步间隔这段时间，切换与turms-gateway的TCP连接，进行DDoS攻击。

  * 通常情况下，您应该将SSL证书放在turms-gateway的上游服务端，即上游的SLB服务或Nginx服务端等。

  * 由于turms-gateway采用了无状态的架构设计，因此任意客户端可以连接到任意一个turms-gateway服务端上，您也可以弹性增删turms-gateway节点，以实现弹性水平拓展；状态（即用户会话信息）被转移到了分布式内存Redis服务端当中。

* 客户端拿到IP地址，并与turms-gateway成功建立TCP连接之后，turms-gateway会检测该IP是否已被封禁，或者turms-gateway自身负载是否过大，如果是，则主动断开TCP连接。否则，放行TCP连接。

* 如果turms-gateway放行TCP连接，

  * 对于使用纯TCP连接的Turms客户端，客户端可以开始发起`TurmsRequest`的Protobuf数据流。该数据流由ZigZag编码的`正文长度`头，与Protobuf编码的`正文`，这两部分组成。
  * 对于使用WebSocket连接的Turms客户端，客户端会在TCP连接建立成功后，向turms-gateway发起HTTP Upgrade请求，请求将HTTP Upgrade成WebSocket协议。如果升级成功，客户端就可以把Protobuf编码的`TurmsRequest`数据放在WebSocket Binary Frame的正文中，并发送给turms-gateway。

  注意：这时Turms客户端只是与turms-gateway建立的网络层连接，但用户尚未`登陆`，也并没有建立`会话信息`。

* 该数据流经过负载均衡服务端（可选）的转发后，会先到达turms-gateway。turms-gateway会先对该数据流进行简单的Protobuf格式校验（不校验具体业务请求的合法性，是为了与turms服务端进行业务逻辑解耦，以实现turms服务端对业务请求格式进行更新后，turms-gateway不需要停机），如果是非法数据流，则直接断开TCP连接。

  否则，若为合法请求，则会对其进行部分解析，以确认turms-gateway能否自行处理这个请求。

* 如果能够自行处理，则在处理后返回响应。如果无法处理，则再检测用户是否已在本机登陆，如果没有登陆，则拒绝执行请求，并发回响应。如果已登陆，则先根据负载均衡策略从可用的turms-service服务端列表中选出一个turms-service服务端，再通过自研的RPC框架将请求转发给该turms-service服务端，让其进行处理。

  * 如果turms-gateway检测到该客户端请求是`登陆请求`，则turms-gateway会根据`用户ID`与请求登陆的`设备类型`构成一个`会话ID`，并根据Redis或本地缓存中的用户会话信息，判断该会话ID是否与已登陆会话冲突。如果发生冲突，则拒绝其进行上线操作，并发回响应，告知客户端被拒绝登陆的原因。否则，将当前用户会话信息注册到Redis，并发回登陆成功响应。此时，用户进入了`在线`状态。

    注意：一个会话ID（用户ID+设备）在同一时刻只会与一个turms-gateway服务端构成`用户会话`，与一个turms-gateway服务端构成TCP连接。用户后续的所有业务请求都是在这一个会话与TCP连接中完成的，直到会话关闭、用户下线。

  * 如果turms-gateway无法处理该客户端请求，则通过RPC服务将客户端请求下发给turms-service。turms-service服务端在收到客户端请求后，会对请求进行校验与处理，并触发`ClientRequestHandler`插件以协助开发者实现自定义逻辑（如敏感词过滤），另外在处理过程中通常也会向mongos发送对应的CRUD请求。等用户请求处理完毕后，turms-service会将产生的`响应`，发回给turms-gateway。对于处理过程中产生的`通知`，turms-service会先根据被通知用户的ID，向Redis或本地缓存查询该批用户所连接的turms-gateway的节点ID，并通过RPC服务将通知发送给这批turms-gateway，让其进行通知下推操作。

    补充：Turms采用MongoDB的分片副本架构。mongos收到CRUD请求后，会根据配置进行CRUD请求路由。

  * 无论turms-gateway接收到的是响应还是通知，turms-gateway都不会对其进行合法性校验，而是直接透传给用户。在通知下推过程中，turms-gateway会触发`NotificationHandler`插件方法以协助开发者实现自定义逻辑（如离线用户的消息推送）。

  （值得一提的是，Turms的所有网络IO操作都是基于Netty实现的，即以上所有RPC、数据库调用均是异步非阻塞的）
